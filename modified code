!mkdir -p /home/aistudio/.cache/paddle/dataset/imdb/
!cp /home/aistudio/data/data69/aclImdb_v1.tar.gz /home/aistudio/.cache/paddle/dataset/imdb/
print('完成')

In [1]
import os
import io
import json
import random
import zipfile
import numpy as np
import matplotlib.pyplot as plt
import paddle
import paddle.fluid as fluid
from paddle.fluid.dygraph.nn import Conv2D, Pool2D, Linear, Embedding
from paddle.fluid.dygraph.base import to_variable
from paddle.fluid.dygraph import GRUUnit
import paddle.dataset.imdb as imdb

In [2]

def load_vocab():

    vocab = imdb.word_dict()

    return vocab
In [ ]
class SentaProcessor(object):
    def __init__(self):
        self.vocab = load_vocab()

    def data_generator(self, batch_size, phase='train'):
        if phase == "train":
            return paddle.batch(paddle.reader.shuffle(imdb.train(self.vocab),25000), batch_size, drop_last=True)
        elif phase == "eval":
            return paddle.batch(imdb.test(self.vocab), batch_size,drop_last=True)
        else:
            raise ValueError(
                "Unknown phase, which should be in ['train', 'eval']")
In [ ]
class DynamicGRU(fluid.dygraph.Layer):
    def __init__(self,
                 size,
                 param_attr=None,
                 bias_attr=None,
                 is_reverse=False,
                 gate_activation='sigmoid',
                 candidate_activation='tanh',
                 h_0=None,
                 origin_mode=False,
                 init_size = None):
        super(DynamicGRU, self).__init__()
        self.gru_unit = GRUUnit(
            size * 3,
            param_attr=param_attr,
            bias_attr=bias_attr,
            activation=candidate_activation,
            gate_activation=gate_activation,
            origin_mode=origin_mode)
        self.size = size
        self.h_0 = h_0
        self.is_reverse = is_reverse
    def forward(self, inputs):
        hidden = self.h_0
        res = []
        for i in range(inputs.shape[1]):
            if self.is_reverse:
                i = inputs.shape[1] - 1 - i
            input_ = inputs[ :, i:i+1, :]
            input_ = fluid.layers.reshape(input_, [-1, input_.shape[2]], inplace=False)
            hidden, reset, gate = self.gru_unit(input_, hidden)
            hidden_ = fluid.layers.reshape(hidden, [-1, 1, hidden.shape[1]], inplace=False)
            res.append(hidden_)
        if self.is_reverse:
            res = res[::-1]
        res = fluid.layers.concat(res, axis=1)
        return res
In [ ]
class GRU(fluid.dygraph.Layer):
    def __init__(self):
        super(GRU, self).__init__()
        self.dict_dim = train_parameters["vocab_size"]
        self.emb_dim = 128
        self.hid_dim = 128
        self.fc_hid_dim = 96
        self.class_dim = 2
        self.batch_size = train_parameters["batch_size"]
        self.seq_len = train_parameters["padding_size"]
        self.embedding = Embedding(
            size=[self.dict_dim + 1, self.emb_dim],
            dtype='float32',
            param_attr=fluid.ParamAttr(learning_rate=30),
            is_sparse=False)
        h_0 = np.zeros((self.batch_size, self.hid_dim), dtype="float32")
        h_0 = to_variable(h_0)
        self._fc1 = Linear(input_dim = self.hid_dim, output_dim=self.hid_dim*3)
        self._fc2 = Linear(input_dim=self.hid_dim, output_dim=self.fc_hid_dim, act="tanh")
        self._fc_prediction = Linear(input_dim=self.fc_hid_dim,
                                 output_dim=self.class_dim,
                                 act="softmax")
        self._gru = DynamicGRU( size= self.hid_dim, h_0=h_0)
    def forward(self, inputs, label=None):
        emb = self.embedding(inputs)
        o_np_mask =to_variable(inputs.numpy().reshape(-1,1) != self.dict_dim).astype('float32')
        mask_emb = fluid.layers.expand(
            to_variable(o_np_mask), [1, self.hid_dim])
        emb = emb * mask_emb
        emb = fluid.layers.reshape(emb, shape=[self.batch_size, -1, self.hid_dim])
        fc_1 = self._fc1(emb)
        gru_hidden = self._gru(fc_1)
        gru_hidden = fluid.layers.reduce_max(gru_hidden, dim=1)
        tanh_1 = fluid.layers.tanh(gru_hidden)
        fc_2 = self._fc2(tanh_1)
        prediction = self._fc_prediction(fc_2)
        if label is not None:
            acc = fluid.layers.accuracy(prediction, label=label)
            return prediction, acc
        else:
            return prediction
In [ ]

train_parameters = {
    "epoch": 1,                              #训练轮次
    "batch_size": 32,                        #批次大小
    "lr":0.002,                              #学习率
    "padding_size": 150,                     #padding纬度
    "vocab_size": 89527,                     #padding的值
    "skip_steps":50,                         #每10个批次输出一次结果
    "save_steps": 150,                       #300个批次保存一次
    "checkpoints":"data/"                    #训练时每个批次的大小
}
In [33]
def train():
    with fluid.dygraph.guard(place = fluid.CUDAPlace(0)):
    # with fluid.dygraph.guard(place = fluid.CPUPlace()):

        processor = SentaProcessor()
        train_data_generator = processor.data_generator(batch_size=train_parameters["batch_size"], phase='train')

        model = GRU()
        sgd_optimizer = fluid.optimizer.Adagrad(learning_rate=train_parameters["lr"],parameter_list=model.parameters())

        steps = 0
        total_cost, total_acc = [], []
        for eop in range(train_parameters["epoch"]):
            for batch_id, data in enumerate(train_data_generator()):

                steps += 1
                doc = to_variable(
                    np.array([
                        np.pad(x[0][0:train_parameters["padding_size"]], 
                              (0, train_parameters["padding_size"] - len(x[0][0:train_parameters["padding_size"]])),
                               'constant',
                              constant_values=(train_parameters["vocab_size"]))
                        for x in data
                    ]).astype('int64').reshape(-1))
                label = to_variable(
                    np.array([x[1] for x in data]).astype('int64').reshape(
                        train_parameters["batch_size"], 1))
        
                model.train()
                prediction, acc = model(doc, label)
                loss = fluid.layers.cross_entropy(prediction, label)
                avg_loss = fluid.layers.mean(loss)
                avg_loss.backward()
                sgd_optimizer.minimize(avg_loss)
                model.clear_gradients()
                total_cost.append(avg_loss.numpy()[0])
                total_acc.append(acc.numpy()[0])
 
                if steps % train_parameters["skip_steps"] == 0:
                    print("step: %d, ave loss: %f, ave acc: %f" %
                         (steps,avg_loss.numpy(),acc.numpy()))

                if steps % train_parameters["save_steps"] == 0:
                    save_path = train_parameters["checkpoints"]+"/"+"save_dir_" + str(steps)
                    print('save model to: ' + save_path)
                    fluid.dygraph.save_dygraph(model.state_dict(),
                                                   save_path)
In [34]
train()
In [ ]
def to_eval():
    with fluid.dygraph.guard(place = fluid.CUDAPlace(0)):
        processor = SentaProcessor()

        eval_data_generator = processor.data_generator(batch_size=train_parameters["batch_size"],phase='eval')
     
        model_eval = GRU()
        model, _ = fluid.load_dygraph("data/save_dir_750.pdparams")
        model_eval.load_dict(model)

        model_eval.eval()
        total_eval_cost, total_eval_acc = [], []
        for eval_batch_id, eval_data in enumerate(eval_data_generator()):
            eval_np_doc = np.array([np.pad(x[0][0:train_parameters["padding_size"]],
                                    (0, train_parameters["padding_size"] -len(x[0][0:train_parameters["padding_size"]])),
                                    'constant',
                                    constant_values=(train_parameters["vocab_size"]))
                            for x in eval_data
                            ]).astype('int64').reshape(-1)
            eval_label = to_variable(
                                    np.array([x[1] for x in eval_data]).astype(
                                    'int64').reshape(train_parameters["batch_size"], 1))
            eval_doc = to_variable(eval_np_doc)
            eval_prediction, eval_acc = model_eval(eval_doc, eval_label)
            loss = fluid.layers.cross_entropy(eval_prediction, eval_label)
            avg_loss = fluid.layers.mean(loss)
            total_eval_cost.append(avg_loss.numpy()[0])
            total_eval_acc.append(eval_acc.numpy()[0])

        print("Final validation result: ave loss: %f, ave acc: %f" %
            (np.mean(total_eval_cost), np.mean(total_eval_acc) ))
In [ ]
to_eval()
In [ ]
def load_data(sentences):
    lod = []
    word_dict = imdb.word_dict()
    UNK = word_dict['<unk>']
    reviews = [c.split() for c in sentences]
    for c in reviews:
        for words in c :
            lod.append(word_dict.get(words.encode('utf-8'), UNK))
    return lod
In [ ]
train_parameters["batch_size"] = 1

with fluid.dygraph.guard(place = fluid.CUDAPlace(0)):

    sentences = 'this is a great movie'
    data = load_data(sentences)
    print(data)
    data_np = np.array(data)
    data_np = np.array(np.pad(data_np,(0,150-len(data_np)),"constant",constant_values =train_parameters["vocab_size"])).astype('int64').reshape(-1)
    infer_np_doc = to_variable(data_np)

    model_infer = GRU()
    model, _ = fluid.load_dygraph("data/save_dir_750.pdparams")
    model_infer.load_dict(model)
    model_infer.eval()
    result = model_infer(infer_np_doc)
    print('预测结果为：正面概率为：%0.5f，负面概率为：%0.5f' % (result.numpy()[0][0],result.numpy()[0][1]))
